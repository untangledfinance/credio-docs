1. Overview
* Source code: https://github.com/untangledfinance/spark-crawler-decoder
* Technical: Spark
* Language: Scalar
* Features:
    * Download raw data from RPC and save to data warehouse
    * Decode raw data into call and event tables according to the provided ABI.
    * Get information of contracts from RPC, for example: names, symbols, decimals of ERC20
    * Can execute on multiple nodes of an AWS EMR cluster, allowing for parallel and distributed processing to improve efficiency.
    * Download both past and new data at the same time.
    * Automatically decode data when a new ABI is added.
* Execution
    * Run by spark-submit on EMR cluster
    * Controlled and scheduled by Scheduler: Airflow

2. Jobs in project
* Crawler: Job downloads data from RPC Service and stores it in the blocks table (raw transactions and logs data) and block_trace table (raw traces data)
* ExtractLog: Extract log data from blocks table and save into logs table
* ExtractTransaction: Extract transaction data from blocks table save into transactions table
* ExtraceTrace: Extract trace data from block_traces table and save into trace table
* DecodeLog: Decode log data in logs table and save into event table. Example: erc20_evt_transfer
* DecodeTransaction: Decode trace data in traces table and save into call table. Example: centrifuge_shelf_call_borrow 
* GetContractInfo: Using RPC to get information of contract from event or call table and save into info table: Example: erc20_tokens contains information of all erc20 token which have contract address in erc20_evt_transfer table.
* AllInOne: Includes: Crawler blocks, ExtractLog, ExtractTransaction, DecodeLog, GetContractInfo
* AllInOneTrace: Includes: Crawler block_trace, ExtractTrace, DecodeTransaction

3. Application Properties
* app_name: Name of Job, was used by JobFactory to identify what job will be executed.
* rpc_list: List of RPC url for one chain, were used by Crawler and GetContractInfo
* rpc_list_for_trace: List of RPC url use for one chain, were use by Crawer to get block_trace data.
* max_retry: Max number retries time to query data from RPC urls when request is fail
* number_partitions: Number of data partitions can be processed in parallel
* chain_name: name of chain
* postgres_url: PostgresSQL database url to get ABI of contract which need to decode data.
* postgres_user: user access to postgres database
* postgres_password: password access to postgres database
* protocol_table: table in postgres database store ABI of protocols
* protocol_ids: list of id protocol need to decode for current chain
* protocol_info_table: table in postgres database store attribute of protocol we need get from contract info.
* max_number_block_in_time: max number block can be processed in concurrent
* max_time_run: Number of loops run in 1 run, if 0 is forever loop
* run_history: if False mean data in history of chain will not be processed

4. Schedule job
* All job of this Application will be managed, scheduled in Airflow
5. Example for how to decode new datasets of protocol