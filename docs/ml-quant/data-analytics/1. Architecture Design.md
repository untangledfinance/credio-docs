<img src="/img/ml-quant/architecture-design.png" alt="" />
* Data warehouse: All system data is stored in Data warehouse tables
* Download and decode: Spark jobs download data from RPC service and save raw data to data warehouse, decode raw data into source data (calls and events) using ABI protocol in CMS database.
* Admin CMS: The CMS manages the ABIs of protocols that need to be decoded data, ABI data is stored in the CMS Database.
* Transformer: This is a DBT project used to transform data source tables into a Business table, which plays the same role as the Spellbook on Dune.
* Airflow: The role of administration and scheduling, arranging and coordinating the work of the entire system.
* Query Engine: SQL Server, receives queries from BI tool (Metabase) reads data in data warehouse, calculates, summarizes and returns results. We are using Trino Query Engine run on EMR, read AWS service to get detail
* BI Tool (Metabase): The user interface allows users to write SQL queries to draw charts and build dashboards. The BI application's data (Metabase) is stored in a postgres database.